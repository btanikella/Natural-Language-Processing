{
 "metadata": {
  "name": "",
  "signature": "sha256:8b5c6fa2a919c1a599c7062a2129797049928c5a55193889342b90c167bbbd92"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Project 4. Latent Semantic Analysis and Semi-Supervised Learning #\n",
      "\n",
      "In this assignment, you will use singular value decomposition (SVD) to learn about lexical semantics. You will have to work with \"big\" data: big enough that you will have to think carefully about speed and memory. Of particular importance will **sparse** matrix representations of your data. Some particular functions and classes you might need:\n",
      "\n",
      "- [scipy.sparse.csr_matrix](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) - matrix in compressed sparse row format\n",
      "- [scipy.sparse.diags](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.diags.html) - method for creating sparse diagonal matrices\n",
      "- [diagonal()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.diagonal.html) - get the diagonal of a matrix\n",
      "- [sklearn.preprocessing.normalize](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) - efficiently normalize sparse matrices\n",
      "- [scipy.sparse.csr_matrix.asfptype()](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.asfptype.html) - upcast matrix to a floating point format\n",
      "- [scipy.cluster.vq.kmeans2](http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans2.html#scipy.cluster.vq.kmeans2) - Classify a set of observations into k clusters using the k-means algorithm\n",
      "- [numpy.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) - returns the indices that would sort an array\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "from scipy.sparse.linalg import svds\n",
      "from scipy.sparse import hstack, diags, csr_matrix\n",
      "from sklearn.preprocessing import normalize\n",
      "import numpy as np\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def csv2csr(filename):\n",
      "    word = []\n",
      "    context = []\n",
      "    count = []\n",
      "    with open(filename,'rb') as infile:\n",
      "        reader = csv.reader(infile)\n",
      "        for row in reader:\n",
      "            word.append(int(row[0]))\n",
      "            context.append(int(row[1]))\n",
      "            count.append(int(row[2]))\n",
      "    return csr_matrix((count,(word,context)))\n",
      "\n",
      "def readVocab(filename):\n",
      "    vocab = []\n",
      "    with open(filename,'rb') as vocabfile:\n",
      "        for line in vocabfile:\n",
      "            vocab.append(line.split()[0])\n",
      "    index = dict(zip(range(0,len(vocab)),vocab)) #from numbers to words\n",
      "    inv_index = {j:i for i,j in index.items()} #from words to numbers\n",
      "    return index,inv_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Loading the Data ##\n",
      "\n",
      "Call **C=proj4_starter.csv2csr('doc_trips.csv')** to load a sparse matrix $C$ of a word-document counts. The cell $c[i,j]$ should \n",
      "hold the count of word $i$ in document $j$.\n",
      "\n",
      "Call **idx, iidx=proj4_starter.readVocab('vocab.10k')** to load the vocabulary. You get two **dict** objects, mapping between words \n",
      "and indices in the matrix $C$. In **C[iidx['Obama'],:]**, you have the document counts for the word *Obama*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Load data and dictionaries\n",
      "'''\n",
      "C = csv2csr('doc_trips.csv')\n",
      "idx, iidx = readVocab('vocab.10k')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Cosine Similarity ##\n",
      "\n",
      "The *cosine similarity* of two vectors $u$ and $v$ is defined as \n",
      "$$\\frac{\\sum_{i}u_iv_i}{\\sqrt{\\sum_iu_i^2\\sum_iv_i^2}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 1** Consider the words *coffee, play, crazy, facebook*, and *hermana* (Spanish for *sister*). For each of them, find \n",
      "the 10 most similar words according to cosine similarity of the rows in $C$.\n",
      "\n",
      "**Hint** The size of the vocabulary is nearly 10,000 words. You do not want to compute and store the entire $10K\\times 10K$ matrix \n",
      "of cosine similarities. Rather, you want to compute them on demand for a given row of the matrix. You may also want to do some\n",
      "precomputation to take care of denominator in advance. Whatever you do, don't lose the sparsity of $C$, or you will not be able \n",
      "to store it.\n",
      "\n",
      "**Sanity check** For *facebook*, the top 5 words I get are *facebook page on twitter deleted*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here is the word list\n",
      "word_list = ['coffee','play','crazy','facebook','hermana']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalizeRow(x):\n",
      "    '''\n",
      "    Normalize each row of x\n",
      "    \n",
      "    Args:\n",
      "    x - data matrix\n",
      "    Return:\n",
      "    row-normalized x\n",
      "    '''\n",
      "    return diags(np.array(1./(1e-6+np.sqrt(x.multiply(x).sum(axis=1))))[:,0],0) * x\n",
      "\n",
      "def computeCosSimPerWord(word_idx, x):\n",
      "    '''\n",
      "    For a given data matrix, compute cosine similarity between the word vocab[word_index] and all words \n",
      "    (including itself)\n",
      "    \n",
      "    Args:\n",
      "    word_idx - the index of one word\n",
      "    x - data matrix\n",
      "    Return:\n",
      "    The cosine similarity between x[word_idx,:] and all other words\n",
      "    '''\n",
      "    finalResult = [None]*len(iidx)\n",
      "    u= x[word_idx,:]\n",
      "    v=0\n",
      "    dU= x[word_idx,:].multiply(x[word_idx,:]).sum()\n",
      "    dV= 0\n",
      "    for i in range(len(iidx)):\n",
      "        v= x[i,:]\n",
      "        dV= v.multiply(v).sum()\n",
      "        numerator= v.multiply(u).sum()\n",
      "        denominator = np.sqrt(dV*dU)\n",
      "        finalResult[i] = numerator/denominator\n",
      "    return np.array(finalResult)\n",
      "\n",
      "# u= x[word_idx,:]\n",
      "#     finalResult = u.dot(x.transpose())  This will give the cosine similarity values. I am not able to change the matrix to np.array.  \n",
      "        \n",
      "#     return np.array(finalResult.todense())\n",
      "        \n",
      "normalizedC = normalizeRow(C)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def printSimilarWords(x, word_list, sim_func, vocab=idx, ivocab=iidx):\n",
      "    '''\n",
      "    Print similar words for each word in \"word_list\". \n",
      "    The similarity between words are computed from data matrix \"x\"\n",
      "    The way of measuring similarity is defined in \"sim_func\"\n",
      "    '''\n",
      "    for word in word_list:\n",
      "        print word, ':', \n",
      "        word_idx = ivocab[word]\n",
      "        sim_idx = np.argsort(-sim_func(word_idx, x))[:10]\n",
      "        for word2_idx in sim_idx:\n",
      "            print vocab[word2_idx],\n",
      "        print ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print the results\n",
      "'''\n",
      "printSimilarWords(normalizedC, word_list, sim_func=computeCosSimPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee mug shop starbucks drinking drink cup cups and large \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play to games game the and with soccer i . \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy 's how that it shit is drives i . \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook page on twitter deleted instagram status compra post whatsapp \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana mi concha la y de tu con regalo que \n"
       ]
      }
     ],
     "prompt_number": 256
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 2 ** Come up with five words of your own that you think might be interesting, and list the top 10 most similar \n",
      "for each. Try to choose a few different types of words, such as verbs, adjectives, names, emotions, abbreviations, or \n",
      "alternative spellings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Add five words to the following list\n",
      "'''\n",
      "additional_word_list = ['tea','british','wow','messi','imagine']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print the results\n",
      "'''\n",
      "printSimilarWords(normalizedC, additional_word_list, sim_func=computeCosSimPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea green cup sweet bubble drinking ice of drink bags \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british accent american bbc india fuk european talent named nearly \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow bow . ! that @u i just so the \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi ronaldo argentina cr7 oro golden cristiano robben mundial player \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine can if could how couldn't would hemmings can't i \n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Document Co-occurence ##\n",
      "\n",
      "Compute the document co-occurence matrix $D$, where $d_{i,j}$ is the probability $P(w_j|w_i)$ that word $j$ appears in a tweet, \n",
      "given that word $i$ appears. To do this, first compute the co-occurence counts $CC^\\top$. Substract the diagonal, then normalize \n",
      "each row. \n",
      "\n",
      "Note: it is possible to smooth this probability, but if you naively add some number to the matrix, you will lose sparsity \n",
      "and memory will blow up. Smoothing is not required for this assignment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 3** For each of the 10 examples above (my five words and your five words), find the 10 most similar words \n",
      "according to cosine similarity of the rows of $D$.\n",
      "\n",
      "**Sanity check** For *facebook*, the 5 words I get are *facebook instagram twitter tv youtube*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Cword_list = ['coffee','play','crazy','facebook','hermana','tea','british','wow','messi','imagine']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeCooccurMatrix(C):\n",
      "    '''\n",
      "    Compute the co-occurence matrix D from C\n",
      "    '''\n",
      "    cooccurence= C.dot(C.transpose()) #CC^T\n",
      "    normalizedCC= cooccurence-diags(cooccurence.diagonal(),0)\n",
      "    return normalizedCC\n",
      "\n",
      "D = computeCooccurMatrix(C)\n",
      "normalizedD = normalizeRow(D)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print the results\n",
      "'''\n",
      "printSimilarWords(normalizedD, Cword_list, sim_func=computeCosSimPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee new today drinking getting food after day eating water \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play down be back pass show beat them up run \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy weird sad not stupid damn actually cool about over \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook instagram twitter tv youtube tumblr 100 note ex insta \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana abuela hermano vieja novio novia viejo padre familia corazon \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea coffee full beer wine water fish new two running \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british by from line against an american coach which they're \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow dude very awesome totally well poor sweet super good \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi final argentina chile leo mayor no metro colombia carlos \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine as with then without for man will fight beat \n"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. Latent Semantic Analysis ##\n",
      "\n",
      "Perform truncated SVD (**scipy.sparse.linalg.svds**) to obtain $\\mathbf{USV}^\\top\\approx \\mathbf{C}$ using $K=10$. \n",
      "Each row vector $\\mathbf{u}_i$\n",
      "is a description of the word $i$. You can compute similarity between pairs of words using the Euclidean norm \n",
      "$\\|\\mathbf{u}_i-\\mathbf{u}_j\\|_2$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4(a)** For each of the 10 examples above, find the 10 most similar words according to Euclidean distance in $\\mathbf{U}$\n",
      "\n",
      "**Sanity check** For *facebook*, the top 5 words are *facebook ex harry calls snap*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeEuclidDistPerWord(word_idx, U):\n",
      "    '''\n",
      "    Compute the Euclid distance bwteen word vocab[word_idx] and all words \n",
      "    (including itself). Then, return the top 10 similar word indices\n",
      "    \n",
      "    Args:\n",
      "      word_idx - the word index\n",
      "      U - latent representation of words\n",
      "    Return:\n",
      "        the **negative**  Euclidean distance between U[word_idx,:] and all other words,\n",
      "        so that more similar words have higher values\n",
      "'''\n",
      "    finalResult = [None]*U[0].shape[0]\n",
      "    u= U[0][word_idx,:]\n",
      "    for i in range(U[0].shape[0]):\n",
      "        uj= U[0][i,:]\n",
      "        value=0\n",
      "        for j in range(len(u)):\n",
      "            value+=(u[j]-uj[j])*(u[j]-uj[j])\n",
      "        value = np.sqrt(value)\n",
      "        finalResult[i]=-value\n",
      "    return np.array(finalResult)\n",
      "        \n",
      "    \n",
      "    #For each row U_i finding the value\n",
      "\n",
      "        \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Once you finish the function computeEuclidDistPerWord, run the following code directly to print results\n",
      "'''\n",
      "Cfp= C.asfptype()\n",
      "U = svds(Cfp, 10)\n",
      "printSimilarWords(U, Cword_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee drinking paper cat eating apparently turned short dress months \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play looking everyone anyone another start coming use enough away \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy gone stupid sad cool yet funny perfect must once \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook ex harry calls snap instagram ...... uh 18 fav \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana unas minutos colegio toca novia verte gana juego unos \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea gold bigger fish record wine student bird basically foot \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british tied events security mothers we'd schools signs oil moyes \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow dude thats either seeing definitely least played dead enjoy \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi madrid mia dale familia mina esperar contra tout frente \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine sense none tweeting pull fuckin deep longer wondering mouth \n"
       ]
      }
     ],
     "prompt_number": 198
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4(b) ** Now compute the same SVD with $K=50$, and again find the 10 most similar words according to Euclidean distance $U$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print results\n",
      "'''\n",
      "U = svds(Cfp, 50)\n",
      "\n",
      "printSimilarWords(U, Cword_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee tea wine four gas midnight starbucks service wedding six \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play hang run chill we're agree nobody kids fight games \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy weird stupid actually af dumb serious dead ugly dude \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook instagram focus ig netflix earth floor fb tl list \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana sonrisa llevo plata jajajaj realidad arriba jajaj vuelta novia \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea wine glass art speaking cards bowl loads sight duty \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british cuts bars rooms foreign dicks caps workers african fights \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow awesome dude omg holy heard needed realized news dumb \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi sido fuera hombre debe medio nosotros primero camino estado \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine someday claim wont bothered successful patient brave he'll held \n"
       ]
      }
     ],
     "prompt_number": 199
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 5 ** Now compute the SVD of the matrix $\\mathbf{D}$, using with $K = 10$, and $K = 50$. Report \n",
      "the most similar words to each of the example words according to Euclidean distance in $U$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print results\n",
      "'''\n",
      "D= D.asfptype()\n",
      "U = svds(D, 10)\n",
      "\n",
      "printSimilarWords(U, Cword_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee upset leaving crap bullshit boring nights loud losing reading \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play anyone use looking move forward stop win went music \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy gone mind forever kind hot full fun sick fucked \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook 24 david alcohol simple 17 traffic 22 priority med \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana entrar esperar juego colegio saben unico culpa minutos unas \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea wine warm states gold basically attitude turning quiet nasty \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british customers remove schools moyes added seniors meat tyler smash \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow cool amazing awesome may tho maybe very mr both \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi serio lunes familia noite coisas merda onda sonrisa amar \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine random ran appreciate touch pics ive thinks dropped wrote \n"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to print results\n",
      "'''\n",
      "U = svds(D, 50)\n",
      "\n",
      "printSimilarWords(U, Cword_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee cream tea ice cheese starbucks chicken chips cake pack \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play run drive beach beat party walk ball join stay \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy af weird dumb dying freaking gay crying confused stupid \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook instagram ig netflix focus couch nerves insta floor fb \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana vieja verga abuela peli loca ropa tarea prueba siesta \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea highlight wine large art milk parts artist chips shades \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british foreign southern ukraine wave hidden cuts 1% criminal canadian \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow awesome omg hahah dude yay tho lmao lucky bet \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi gobierno futbol pueblo camino contra programa agua primer estado \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine known legend wins consider would've decent decision winner dollar \n"
       ]
      }
     ],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U = svds(D, 100)\n",
      "\n",
      "printSimilarWords(U, Cword_list, sim_func=computeEuclidDistPerWord)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee tea wine cheese chicken glass workout sandwich milk truck \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play players played against ball agree lebron team player beat \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy weird fast boring fake bullshit stupid ugly lame dumb \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook instagram netflix focus ig floor earth couch list nerves \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana llevar peli abuela plata vieja chica novia hija verga \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea glass fish chips size milk loads fruit wine whiskey \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british foreign robot castle policy former anthony secretly served agent \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow awesome bong dude super yay hahahahaha omg okc exactly \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi ronaldo suarez costa juan david trabajo diego paul barca \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine afford spell describe breathe explain mistake relate guarantee escape \n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 6 ** Overall, which set of synonyms looks best to you? \n",
      "Count how many of the top 10 synonyms for each of the 5 example words \n",
      "(the ones I picked) have the same majority part of speech (e.g., *play* is a verb) as the cue word.\n",
      "Use the tagset from the [Twitter POS paper](http://www.cc.gatech.edu/~jeisenst/papers/acl2012pos.pdf)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The best set of synonyms are produced when the K=50 was used for the D and C matrices. However the best pick would be in the SVD truncated D matrix. Here is the list of the number of top 10 synonyms, which share the same part of speech. \n",
      "\n",
      "Coffee [N(NN)] -> 9 similar NN, except Starbucks which is considered to be a proper noun ^. However Coffee is very similar to Starbucks and therefore can be considered to be good in the set of synonyms. \n",
      "\n",
      "Play [N(NN)] -> 8 similar NN tags, with two Verb base form which are join and stay. \n",
      "\n",
      "Crazy [A(JJ)] -> 5 similar Adjective tags, the remaining 5 tags are af, which is ambiguous considering it being a acronym, it is NN. Dying, Crying, Freaking are Verbs with present participles and confused is a verb with past participle. \n",
      "\n",
      "Facebook [N(NN)] -> 10 same pos tags. All the resulting synonyms are of the same tag as the cue word Facebook. \n",
      "\n",
      "Hermana [N(NN)] -> 10 same pos tags. All the resulting synonyms are similar to that of the cue word Hermana. Even though it is in Espanol. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5. Local Context ##\n",
      "\n",
      "Local context captures the frequency with which words appear in each others\u2019 immediate context. We have provided a CSV file (succ_trips_50k.csv)  in which each line contains a triple $\\langle x,y,z\\rangle$, \n",
      "where $x$ and $y$ are term IDs and $z$ is the count of times where $y$ immediately follows $x$ . \n",
      "The vocabulary has now increased to 50K words. There is an associated vocabulary file, **vocab.50k**.\n",
      "\n",
      "Build a sparse matrix $\\mathbf{E}$ from these triples. Normalize the rows of $\\mathbf{E}$, such that $e_{i,j}=\\frac{n(i,j)}{n(i)}$, \n",
      "the probability of seeing word $j$ given that you have just seen word $i$. \n",
      "Mow form a matrix $\\mathbf{F} = [\\mathbf{E}~ \\mathbf{E}\u2019]$ by horizontally concatenating the normalized matrix $\\mathbf{E}$. \n",
      "You will perform sparse singular value decomposition on $\\mathbf{F}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalizeRow50(E):\n",
      "#     temp = E\n",
      "#     for i in range(E.shape[0]):\n",
      "#         temp[i,:]=(temp[i,:]*1.0)/(1e-6+(temp[i,:].sum()*1.0))\n",
      "#     return temp\n",
      "    arrayValue=np.array(1./(1e-6+E.sum(axis=1)))\n",
      "    \n",
      "    return diags(arrayValue[:,0],0)*(E)\n",
      "\n",
      "        \n",
      "def constructF(E):\n",
      "    '''\n",
      "    Finish the following code to construct F from E\n",
      "    '''\n",
      "    ## Your code here\n",
      "    F= normalizeRow50(E)\n",
      "    \n",
      "    \n",
      "    return F\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Load new dataset and the corresponding dictionaries\n",
      "'''\n",
      "idx_50, iidx_50 = readVocab('vocab.50k')\n",
      "E = csv2csr('succ_trips_50k.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.array(1./(1e-6+E.sum(axis=1)))[:,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.00666667  0.01369863  0.02941176 ...,  0.14285712  0.03333333\n",
        "  0.00819672]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "F = constructF(E)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 7 ** For $K = 10$ and $K = 100$ compute the top 10 synonyms for each of your ten words. \n",
      "\n",
      "** Sanity check ** For *facebook*, I get: *facebook cnn duncan nicole consistent* from $K = 10$\n",
      "and *facebook instagram youtube ig twitter* from $K = 100$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to get the results with K=10\n",
      "'''\n",
      "U_f = svds(F, 10)\n",
      "printSimilarWords(U_f, Cword_list, sim_func=computeEuclidDistPerWord, vocab=idx_50, ivocab=iidx_50)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee finger tax quiet speech friday coach common scoring cold \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play fall lay comes pay rely kick lose live perform \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy reported smiling crying beth sucked died crashed suspended kane \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook cnn duncan nicole consistent ronaldo smith suarez injured bio \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana aun estaba se aguanta pega duran todavia tampoco estudie \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea network cheese tech empty pals shit evil scoring research \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british lunchtime lower committee using belly japanese living topping housing \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow okayyy woah indeed woof oii alright rofl hahaaaa hellooo \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi nova leo acabando angela sofia pablo titan rola brasil \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine bless tape murdered funnier smiling normally suspicious tf shii \n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Run the following code to get the results with K=100\n",
      "'''\n",
      "U_f = svds(F,100)\n",
      "printSimilarWords(U_f, Cword_list, sim_func=computeEuclidDistPerWord, vocab=idx_50, ivocab=iidx_50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coffee pizza tea beer food softball fruit curry milk alcohol \n",
        "play : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "play drive dance fly survive be swim murder sing win \n",
        "crazy : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crazy scary who cool gay else everyone cooler elton that \n",
        "facebook : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "facebook instagram youtube ig twitter tumblr wednesday netflix holiday wallay \n",
        "hermana : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hermana abuela hermano hna piace amorcito sobrina yetmez tocca sobrino \n",
        "tea : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tea hours coffee cup impractical state round golf gossip years \n",
        "british : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "british white green human mexican black navy park wild neon \n",
        "wow : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "wow dude lol lmfao omg lmao omfg aha lls bruh \n",
        "messi : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "messi juan argentina pablo al david ronaldo robben bieber #0ff \n",
        "imagine : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "imagine won than !!!!!!!!!!!!! tweet understand seen fwm dude ever \n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 8 ** Again, count the proportion of proposed synonyms that have the same POS as \n",
      "the cue words that I selected. Does local context or document context do better at matching the POS of the cue words? Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The best returning values are the result of SVD truncation of the document context matrix with the K being 100. When compared with the results of Document context matrix (D) with the probabilistically normalized matrix F, the best resulting synonyms were that of Document Context. \n",
      "\n",
      "Results for U_F\n",
      "\n",
      "Coffee [N(NN)] -> 10 similar pos tags. However the only discrepancy can occur with curry, which can be considered either a noun or a verb depending on the context. \n",
      "\n",
      "Play [N(NN)] -> 4 common Nouns (NN). The discrepencies being fly->Verb V, survive ->  V Verb, be -> V verb, swim-> V verb, sing -> Verb V, win -> can be considered very similar to play according to context, Verb V. \n",
      "\n",
      "Crazy [A(JJ)] -> 5 common pos tags. The discrepancies are [who, else, everyone, elton, that], these are considered to be Pronoun, Adverb, Common Noun, Proper Noun, Preposition; respectively. \n",
      "\n",
      "Facebook [N(NN)] -> 9 common pos tags, the only discrepancy is through the word wallay, which can be considered to be a Verb. \n",
      "\n",
      "Hermana [N(NN)] -> 10 common pos tags. \n",
      "\n",
      "Similarly when checked for the same valuation of K =100 the Document context produced a values of Coffee-10, Play -6, Crazy -9, Facebook - 10, Hermana -10. Therefore when compared the Document context does better at matching the POS tagger of the cue word. The reason document context is better in this instance is due to the fact that Document Context provides the normalized coocurence counts, through which the SVD is truncated to provide a better synonym. However Local Context also has to deal with the fact that it has to go through 50 times more words than the Document context as well as the probablistic normalization can be an efficient way to predict as it is not completely attainable to the training data. The difference between both the Local and the Document Contexts is very minimialistic if considered with all the same variables. However in this instance the Document Context has better results in matching the parts of speech. Overall through the regression model the Local Context have a better equal spread and does a better job at picking out the similarities without any skews. Therefore the Local Context has a better qualitative edge over the Document Context since it can deal with a larger vocabulary and it deals with the probablistic relationship between two words and their context rather than the document based relationship. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 6. Clustering ##\n",
      "\n",
      "Perform KMeans clustering on the rows of the latent semantic representation $US$ that you obtained in the previous section \n",
      "(recall that $U$ describes the words; $S$ scales the latent factors by importance).\n",
      "\n",
      "Let the number of clusters equal 200. You may use an existing KMeans clustering library, \n",
      "or you may write your own. I used **scipy.cluster.vq.kmeans2**, and manually set the number of iterations to 100.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.cluster.vq import kmeans2, kmeans\n",
      "\n",
      "n_clusters = 200\n",
      "'''\n",
      "Your code here for clustering \n",
      "'''\n",
      "centroid,label=kmeans2(U[0].dot(U[1]),n_clusters,iter=100)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 9(a)**  Use pyplot.hist to make a [histogram](http://en.wikipedia.org/wiki/Histogram) of the sizes of each cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import matplotlib.pyplot as plt\n",
      "x= len(label)\n",
      "#somehow compute the sizes\n",
      "sizes = [0]*200\n",
      "for i in range(len(label)):\n",
      "    sizes[label[i]]+=1\n",
      "plt.hist(sizes,color='red')\n",
      "plt.xlabel('Size of Cluster')\n",
      "plt.ylabel('Number of Clusters')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 113, 14, 0, 1, 16, 34, 15, 0, 0, 0, 34, 1, 0, 0, 0, 3, 7, 6, 0, 0, 747, 0, 0, 1, 0, 1, 197, 0, 4, 0, 11, 3, 13, 7, 7, 0, 0, 0, 0, 1611, 11, 5, 1104, 0, 54, 0, 8, 0, 4, 2, 0, 0, 7, 0, 3, 4, 0, 1256, 0, 0, 1, 17, 8, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 0, 7, 3, 0, 9, 1, 34, 4, 0, 0, 6, 7, 7, 0, 8, 1, 0, 224, 520, 39, 2, 250, 0, 0, 0, 3, 1, 5, 2, 0, 59, 7, 138, 0, 0, 0, 0, 0, 0, 11, 0, 0, 2, 2, 0, 15, 0, 7, 10, 13, 0, 0, 9, 0, 0, 4, 0, 0, 0, 5, 4, 4, 7, 4, 3, 1, 0, 2, 90, 11, 0, 574, 3, 0, 0, 0, 1, 2, 0, 0, 8, 15, 6, 0, 0, 0, 11, 0, 0, 94, 6, 74, 3, 1, 6, 1, 4, 0, 2, 4, 0, 2, 1, 6, 7, 0, 0, 6, 4, 11, 1897, 0, 5, 339, 1, 3, 10, 1, 2, 3, 1, 1, 1]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1MzQkCQ1MzQkSc0MDUlSs/8PbtoJ5EC3it8AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x11fd18dd0>"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "centroid,label=kmeans2(U[0],n_clusters,iter=100)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print centroid[-0.79850935]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2021.47924859\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:1: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sizes = [0]*200\n",
      "for i in range(len(label)):\n",
      "    sizes[label[i]]+=1\n",
      "plt.hist(sizes)\n",
      "plt.xlabel('Size of Cluster')\n",
      "plt.ylabel('Number of Clusters')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npSFJamZpSJKaWRqSpGb/H+qXvxNm+3NXAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x13befea50>"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 9(b)** Out of your ten examples words (you can exclude *hermana*), pick the one in the smallest cluster. \n",
      "Examine the words in this cluster, and list three that you think belong, and three that you think do not belong."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The word with the smallest cluster is the word play, since the cluster is very well formed the words which revolve around play, playing fall into this cluster. Therefore making the cluster play small compared to the other. The other words such as wow, messi and british are widely disperesed since the context of the words are required. The three words, which I think needs to belong here are play, playing, players, ball. The three words which do not belong are comes, fly and fall, these words are generic and can be assumed that they can be intercepting through other clusters. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 7650 Only: Semi-Supervised Learning ##\n",
      "\n",
      "Now use the clusters from the previous part as features in your implementation from either pset 2 or pset 3.\n",
      "\n",
      "If you already used other word clusters as features in the bakeoff, you still need to do this part with the clusters that you've built yourself in this problem set."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7.1  POS Tagger ###\n",
      "\n",
      "Add the word clusters as features in your Twitter POS tagger. Start with your best performing tagger from **Project 2** (based on the dev data). Add a feature for each cluster/tag pair, e.g. C174/N, C189/V, etc. You will then compute the accuracy with training sets of various sizes, comparing the performance of your model with and without the cluster features."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 11 (Option A)** Build training sets including the first 50, 100, 200, 500, and 1000 *sentences* (not words). \n",
      "Train your tagger on each training set, using your original features, and plot the accuracy on the development set. \n",
      "Then retrain you tagger, including the new word cluster features, and plot accuracy on the development set on the same plot. \n",
      "Run for at least 10 iterations in each case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Your code here\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You may want to try larger numbers of clusters to improve performance. \n",
      "\n",
      "You can even include the results from multiple clusterings with different numbers of clusters (50, 100, 200, 500, ...), \n",
      "using features like C43-50/N (cluster 43 of 50, with tag N), C377-500/V (cluster 377 of 500, with tag V).\n",
      "\n",
      "You can also use clusters for the features of neighboring words."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7.2 Dependency Parser ###\n",
      "\n",
      "If you could not get your implementation of structured perceptron for Twitter POS tagging to work, \n",
      "and you don\u2019t want to keep trying, you can instead add distributional features to the dependency parser from homework 3.\n",
      "\n",
      "Download the Brown clusters from a [word representation website](http://metaoptimize.com/projects/wordreprs/), \n",
      "using the 1000-cluster model. Follow [this paper](http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf) on word representation, \n",
      "adding features for prefixes of length 4, 6, 10, and 20. \n",
      "The features should capture the prefix of the head and modifier. You may also try \u201chybrid\u201d \n",
      "features that combine a prefix for the head, and the POS tag of  the modifier - and vice versa. \n",
      "See [this paper](http://www.cs.columbia.edu/~mcollins/papers/koo08acl.pdf) for more ideas and details.\n",
      "\n",
      "**Deliverable 11 (Option B)** Building training sets including the first 100, 500, 1000, 3000, and 7596 **sentences** (not words).\n",
      "Train your parser on each training set, using your best features, and plot the dev set accuracy. Then retrain your parser, adding \n",
      "the new Brown cluster features. Plot dev set accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Your code here\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}