{
 "metadata": {
  "name": "",
  "signature": "sha256:9a217533e1577bc87d325e325256b7aed633bf4cfee2543c3207b8f75611d608"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem set 3: Parsing\n",
      "===========\n",
      "\n",
      "This problem set contains two parts. \n",
      "\n",
      "- Grammar design: try to design a CFG to parse sentences on Twitter.\n",
      "- Dependency parsing: design features to learn a high-accuracy dependency parser\n",
      "\n",
      "### Honor policy ###\n",
      "\n",
      "- Your work must be your own. Do not discuss the details of the assignment with other people. \n",
      "- You may of course help each other with understanding the ideas discussed in lecture and the readings, and with basic questions about programming in Python. For example, it is fine to discuss how arc-factored dependency parsing works, or how state-splitting works in CFGs. It is **not acceptable** to discuss how to implement a specific feature for dependency parsing, or how to handle a specific linguistic phenomenon in a CFG. It is unacceptable to share your code.\n",
      "- There are implementations and source code for many machine learning algorithms on the internet. Please write the code for this assignment on your own, without using these external resources."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from parseTwitter import evalParser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 1: CFG Design #\n",
      "\n",
      "Your first job is to design a CFG to parse Twitter. We provide some files:\n",
      "\n",
      "- **oct27.clean.train** contains some short sentences from the Twitter POS tagging dataset, avoiding some difficult tags like URLs and retweets.\n",
      "- **parseTwitter.py** contains some code for running a CFG parser on Twitter, using nltk\n",
      "- **simple.cfg** is a minimal CFG, using some ideas we've seen in class\n",
      "\n",
      "The setup is as follows:\n",
      "\n",
      "- You will see input with real sentences from Twitter, as well as scrambled sentences\n",
      "- The terminal symbols are tags, not words. See [here](http://www.ark.cs.cmu.edu/TweetNLP/annot_guidelines.pdf) for more on the tagnset.\n",
      "\n",
      "Scoring:\n",
      "\n",
      "- If your grammar produces at least one parse for a real sentence, that's a true positive; otherwise it's a false negative\n",
      "- If your grammar produces at least one parse for a scrambled sentence, that's a false negative\n",
      "- Your goal is to get a high f-measure, while producing as few parses per sentence as possible\n",
      "- My minimal grammar gets an F-measure of 9.8% (not very good!), with 1.6 parses per parsed sentence.\n",
      "- evalParser takes an optional argument \"show_fns=True\", which will show you the sentences that you are not successfully parsing, and an optional argument \"show_fps=True\", which will show scrambled sentences that were successfully parsed.\n",
      "\n",
      "We will run a bakeoff to see who can get the highest score on each deliverable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:simple.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "{'f-measure': 0.27777732687186474,\n",
        " 'parses-per-sent': 8.749995625002187,\n",
        " 'precision': 0.4081632569762601,\n",
        " 'recall': 0.21052631578947367}"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 1 ** (5 points): design a grammar that gets an f-measure of at least 0.3. Name it \"lastname-firstname.all.cfg\", submit it with your notebook on T-square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:Tanikella-Bharadwaj.all.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "{'f-measure': 0.39378188150080917,\n",
        " 'parses-per-sent': 198.57889511081706,\n",
        " 'precision': 0.38775509808413167,\n",
        " 'recall': 0.4}"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 2 ** (5 points): design a grammar that gets an f-measure of at least 0.25, with no more than 10 parses per sentence. If your previous grammar already got fewer than 10 parses per sentence, then change it to reduce the number of parses per sentence even further. Name it \"lastname-firstname.10.cfg\", and submit it on t-square."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser(\"file:Tanikella-Bharadwaj.10.cfg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "{'f-measure': 0.27777732687186474,\n",
        " 'parses-per-sent': 8.749995625002187,\n",
        " 'precision': 0.4081632569762601,\n",
        " 'recall': 0.21052631578947367}"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Splitting tags (7650 only) ##\n",
      "\n",
      "The Twitter POS tagset is very coarse-grained. You might be able to make a better parser if you could distinguish between subcategories of tags. In this problem, you will try to do this.\n",
      "\n",
      "- The evalParser() code takes an optional argument \"preprocessor\", which you can use to modify the tags in a sentence.\n",
      "- Your parser can then add special handling for the modified tags.\n",
      "\n",
      "For example, the function below splits the \"P\" tag to have a special tag \"2\", for the word \"to\". The grammar jacob.split.cfg can then use the tag \"2\" as a terminal symbol, slightly improving the f-measure and reducing the number of parses per sentence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def myPreprocess(words,tags):\n",
      "    for i,word in enumerate(words):\n",
      "        if (word.lower() == 'to'):\n",
      "            tags[i] = '2'\n",
      "    return words,tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "evalParser('file:jacob.split.cfg',preprocessor=myPreprocess)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "{'f-measure': 0.3369560204402487,\n",
        " 'parses-per-sent': 6.806449417274382,\n",
        " 'precision': 0.3483146028279258,\n",
        " 'recall': 0.3263157894736842}"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3** (3 points; 7650 only): use tag splitting to improve your parser. \n",
      "\n",
      "- A small improvement like the one I obtained above is sufficient, but use a different split than \"to\". \n",
      "- You may check out [Klein and Manning 2003](nlp.stanford.edu/pubs/unlexicalized-parsing.pdf) for ideas. \n",
      "- Name your grammar \"lastname-firstname.split.cfg\", and submit it on T-square."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 2: Dependency parsing #\n",
      "\n",
      "In this problem, you will work with an arc-factored non-projective dependency parser, which is trained by average perceptron. If you were not confident about your own implementation of averaged structure perceptron, please take a look at the code, in the directories shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('parsing/')\n",
      "\n",
      "import dependency_parser as depp\n",
      "import dependency_features as depf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build a dependency parser with a given feature set\n",
      "dp = depp.DependencyParser(feature_function=depf.DependencyFeatures())\n",
      "dp.read_data(\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 801"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train your parser for ten iterations\n",
      "# These are *unlabeled* accuracies.\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.486 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.477\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.486\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.502\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.503\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.504\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.508\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.510\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.509\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.511\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.492 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.510\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You are going to create a series of subclasses of DependencyFeatures, which has features of your choice. Here is an example, which adds a feature that includes:\n",
      "\n",
      "- The part-of-speech of the head\n",
      "- The word of the modifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexFeats(depf.DependencyFeatures):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexFeats,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)\n",
      "        f = self.getF((k,instance.pos[h],instance.words[m]),add)\n",
      "        ff.append(f)\n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several things to notice about this code.\n",
      "\n",
      "- You start by calling the same function, using the parent class. You can build a chain of feature functions in this way.\n",
      "- $h$ provides the index of the head word of the dependency arc\n",
      "- $m$ provides the index of the modifier word of the dependency arc\n",
      "- You can access the part of speech tags in the instance as instance.pos[i], where i indexes any word token\n",
      "- You can access the words themselves as instance.words[i], where $i$ again indexes the token\n",
      "- To create a feature, you call getF(), with two arguments:\n",
      "    - A feature tuple, which includes an index $k$, and any other information you want -- it need not be a tuple of exactly three items\n",
      "    - An argument \"add\", which you don't need to worry about (but you do need to include)\n",
      "    - Make sure to keep $k$ up-to-date. This prevents collisions in the space of features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's run it\n",
      "dp = depp.DependencyParser(feature_function=LexFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 23019"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.535 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.550\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.585 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.565\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.602 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.615 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.571\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.625 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.631 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.638 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.571\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.640 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.642 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.572\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.646 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.573\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4 ** (3 points): start by adding a feature that computes the distance between the head and the modifier, up to a maximum absolute value of 10.\n",
      "\n",
      "**Sanity check**: you should now have 23039 features. Accuracy should improve substantially."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import sign\n",
      "\n",
      "#Get it Checked"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexDistFeats(LexFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexDistFeats,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)\n",
      "        val=(m-h)\n",
      "        si= sign(m-h)\n",
      "        if (abs(val)> 10):\n",
      "            val = 10\n",
      "        valR= abs(val)*si\n",
      "        f = self.getF((k,valR), add) # your code here\n",
      "        ff.append(f)\n",
      "       \n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=LexDistFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)\n",
      "\n",
      "# parsing/../data/deppars\n",
      "# Number of sentences: 7569\n",
      "# Number of tokens: 75621\n",
      "# Number of words: 11766\n",
      "# Number of pos: 48\n",
      "# Number of features: 23039\n",
      "# Epoch 1 Train: 0.661 Dev: 0.660\n",
      "# Epoch 2 Train: 0.707 Dev: 0.680\n",
      "# Epoch 3 Train: 0.724 Dev: 0.694\n",
      "# Epoch 4 Train: 0.737 Dev: 0.700\n",
      "# Epoch 5 Train: 0.744 Dev: 0.709\n",
      "# Epoch 6 Train: 0.754 Dev: 0.711\n",
      "# Epoch 7 Train: 0.758 Dev: 0.718\n",
      "# Epoch 8 Train: 0.763 Dev: 0.716\n",
      "# Epoch 9 Train: 0.767 Dev: 0.718\n",
      "# Epoch 10 Train: 0.775 Dev: 0.722"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 23039"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.661 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.660\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.707 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.680\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.724 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.694\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.737 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.700\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.744 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.709\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.754 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.711\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.758 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.718\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.763 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.716\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.767 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.718\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.775 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.722\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 5 ** (3 points): now add a feature to LexDistFeats, which includes the POS of the modifier and the word of the head.\n",
      "\n",
      "**Sanity check**: The number of features should roughly double. (Do you see why?)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ans: The way a feature has been represented is by a tupule format with the dependency arcs. The number of features are listed to be 801 which represents the arcs. The reason the features roughly double is due to the fact that we are adding the features which are essentially going through each sentence and words in the sentence. The number of words are essentially 11766 and the combinations between the head and modifier in each sentence will out put with the LexFeats which is 23019. When the distance is added to the feature set there is change of 20 features making it 23039. However the same repetion of pos tags and Word combinations outputs a value of 44546 which essentially is about 6.10 combinations of features per sentence or 1.93 times of 23019. Overall the reason it doubled is due to the fact that the training set is added with word and tag combinations features and the duplicates are ignored."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LexDistFeats2(LexDistFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(LexDistFeats2,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)\n",
      "        f = self.getF((k,instance.pos[m],instance.words[h]), add) # your code here\n",
      "        ff.append(f)\n",
      "        return ff\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=LexDistFeats2())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(5)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 44546"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.681 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.691\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.747 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.712\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.777 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.719\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.799 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.721\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.816 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.734\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Context features ## \n",
      "\n",
      "Add context features that consider the tags adjacent to the head and modifier. You may wish to consider various tag combinations, such as \n",
      "\n",
      " - $\\langle t[h], t[h-1], t[m]\\rangle$: head, head-left, modifier\n",
      " - $\\langle t[h], t[m], t[m+1]\\rangle$: head, modifier, modifier-right\n",
      " - $\\langle t[h], t[h-1], t[m], t[m+1]\\rangle$: head, head-left, modifier, modifier-right\n",
      " - etc\n",
      "\n",
      "Note that you can add more than one feature at a time within create_arc_features(). Watch out for edge cases!\n",
      "\n",
      "** Deliverable 6 ** (5 points):\n",
      "\n",
      "Describe what context feature templates you have added. How do they impact the total number of features? How does \n",
      "it impact the development and training accuracy?\n",
      "\n",
      "** Sanity check **: I added a few basic context features, and this increased dev set accuracy above 80%."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Description: The context features added to the features are. \n",
      "\u27e8t[h],t[h\u22121],t[m]\u27e9: head, head-left, modifier\n",
      "\u27e8t[h],t[m],t[m-1]\u27e9: head, modifier, modifier-left\n",
      "These tags are added to the feature templates which can effect the training and dev accuracy by addition of unigram models. The head, head-left and modifier tags allow the reduction of distance between the modifier and head. And the usage of three words in the features allows us to increase the training and dev accuracy as it accumulate the word combinations. Through these templates the number of features are impacted as the number of features after the three tag combinations are 54902. When the number of tags present is 48 the maximum number of features is 17k (48 Choose 3). However with the template the number of effected tags are around 10k. The dev and training accuracy increases as the feature set increases because the features are highly adapted to the training set.\n",
      "\n",
      "\u27e8t[h],t[m],t[m+1]\u27e9: head, modifier, modifier-right\n",
      "\u27e8t[h],t[h-1],t[m],t[m+1]\u27e9: head, head-left, modifier, modifier-right\n",
      "Another feature templates used is if the modifier index is left to the header. The templates of the features reduces the distance from the header and modifier. Essentially the feeatures above effects the training and dev accuracy by highly adapting to the training set. The way it effects the number of features is through the combinations of pos tags. The four tags combinations allow higher accuracies in the training set. The features above and below allow a funnel type feature templates which makes the accuracies higher than normal. The left most modifier of head tags and the right most tag after the modifier allows a higher level of combinations in return higher level of training accuracies. \n",
      "\n",
      "The below feature sets are very similar to the features above and it happens if the modifier index is greater than tha header index. A funnel type of features are created with a combinations of 4 tags. The number of features are created through the combinations of 48 tags choosing 4 and uses the training set of data to create the features which can predict the dev data. If the training set of data is not constricted, the most optimal number of features can be around 200k. \n",
      "\n",
      "\u27e8t[h],t[h+1],t[m],t[m-1]\u27e9: head, head-right, modifier, modifier-left\n",
      "\u27e8t[h],t[h+1],t[m]\u27e9: head, head-right, modifier\n",
      "\u27e8t[h-1],t[h],t[h+1],t[m]\u27e9: head-left, head, head-right, modifier\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ContextFeats(LexDistFeats2):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(ContextFeats,self).create_arc_features(instance,h,m,add)\n",
      "        k = len(ff)\n",
      "        f = self.getF((k,instance.pos[h],instance.pos[h-1],instance.pos[m]), add) # your code here\n",
      "        ff.append(f)\n",
      "        ff.append(self.getF((len(ff),instance.pos[h],instance.pos[m],instance.pos[m-1]), add))\n",
      "        v = len(ff)\n",
      "        if (m < h):\n",
      "            a = self.getF((v,instance.pos[h],instance.pos[m],instance.pos[m+1]),add)\n",
      "            ff.append(a)\n",
      "            g= len(ff)\n",
      "            b = self.getF((g,instance.pos[h],instance.pos[h-1],instance.pos[m],instance.pos[m+1]),add)\n",
      "            ff.append(b)\n",
      "        if (m > h):\n",
      "            length = len(ff)\n",
      "            n= self.getF((length,instance.pos[h],instance.pos[h+1],instance.pos[m],instance.pos[m-1]),add)\n",
      "            ff.append(n)\n",
      "            le = len(ff)\n",
      "            c= self.getF((length,instance.pos[h],instance.pos[h+1],instance.pos[m]),add)\n",
      "            ff.append(c)\n",
      "            ff.append(self.getF((len(ff),instance.pos[h-1],instance.pos[h],instance.pos[h+1],instance.pos[m]),add))\n",
      "\n",
      "        return ff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=ContextFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 75528"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.795 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.824\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.861 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.847\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.886 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.852\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.902 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.858\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.916 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.863\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.925 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.868\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.932 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.870\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.938 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.873\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.941 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.874\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.948 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.874\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bakeoff! ##\n",
      "\n",
      "Try to develop other features that will further improve performance.\n",
      "Please explain all the features that you have. \n",
      "After identifying your best feature set, run *test()* function from DependencyParser. This will produce a file **data/deppars/english_test.conll.pred**.\n",
      "\n",
      "**Deliverable 7** (5 points): Rename this to **lastname-firstname.conll.pred** and include it in your t-square submission.\n",
      "\n",
      "**Sanity check / challenge**: The best test set performance in Fall 2013 was 87.2%. Can you beat it??\n",
      "\n",
      "**NOTE**: As usual, you are not supposed to look at other code online while doing this problem set. However, you are welcome to search for *research papers* on dependency parsing to get ideas for features that might improve your performance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Description: Additional feats contain the features which finds the distances between header and modifier tags and adds it to the features and also the number of modifiers in a sentence to the left and right are added to the features. Through this we can find the valency information of the dependency parser. The number of features, which are added are about 4k. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class AdditionalFeats(ContextFeats):\n",
      "    def create_arc_features(self,instance,h,m,add=False):\n",
      "        ff = super(AdditionalFeats,self).create_arc_features(instance,h,m,add)\n",
      "        ff.append(self.getF((len(ff),sign(m-h)*min(abs(m-h),10),instance.pos[h],instance.pos[m]),add)) \n",
      "        left =0\n",
      "        right =0\n",
      "        if(m<h):\n",
      "            left +=1\n",
      "        else:\n",
      "            right +=1\n",
      "        ff.append(self.getF((len(ff),left,instance.pos[h]),add))\n",
      "        ff.append(self.getF((len(ff),right,instance.pos[h]),add))\n",
      "        \n",
      "        return ff\n",
      "        \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dp = depp.DependencyParser(feature_function=AdditionalFeats())\n",
      "dp.read_data(\"english\")\n",
      "dp.train_perceptron(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "parsing/../data/deppars\n",
        "Number of sentences: 7569"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of tokens: 75621\n",
        "Number of words: 11766\n",
        "Number of pos: 48\n",
        "Number of features: 79071"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.809 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.828\n",
        "Epoch 2 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.866 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.842\n",
        "Epoch 3 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.885 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.856\n",
        "Epoch 4 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.902 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.859\n",
        "Epoch 5 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.913 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.869\n",
        "Epoch 6 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.923 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.872\n",
        "Epoch 7 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.930 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.875\n",
        "Epoch 8 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.935 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.874\n",
        "Epoch 9 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.940 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.876\n",
        "Epoch 10 Train: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.946 Dev: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.878\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run this\n",
      "dp.test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    }
   ],
   "metadata": {}
  }
 ]
}