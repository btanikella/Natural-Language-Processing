{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Demo of word sense clustering using expectation-maximization.\n",
      "\n",
      "By Jacob Eisenstein in 2013\n",
      "\n",
      "For Georgia Tech CS4650/7650, Natural Language Understanding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!jist word-cluster.ipynb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "https://gist.github.com/86b1018f70d5e9cee0c6\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import reuters, webtext, brown\n",
      "from collections import defaultdict\n",
      "import operator\n",
      "import numpy as np\n",
      "from scipy.misc import logsumexp\n",
      "from scipy.sparse import dok_matrix, csr_matrix\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def buildCorpus(query,corpus=brown):\n",
      "    alldicts = []\n",
      "    allwords = set()\n",
      "    for fileid in corpus.fileids():\n",
      "        #print fileid,\n",
      "        words = [x.lower() for x in corpus.words(fileid)]\n",
      "        #print query in words\n",
      "        if query in words:\n",
      "            counts = defaultdict(int)\n",
      "            for word in words:\n",
      "                counts[word] += 1\n",
      "            allwords = set(counts.keys())\n",
      "            alldicts.append(counts)\n",
      "    return allwords,alldicts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allwords,alldicts = buildCorpus('cut',corpus=brown)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#parameters\n",
      "D = 1000\n",
      "Dmin = 100\n",
      "N = len(alldicts) #num instances\n",
      "K = 2 #num classes\n",
      "alpha = 1.0 #smoothing\n",
      "print D,Dmin,N,K,alpha"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000 100 111 2 1.0\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#compute total counts for each word\n",
      "totcounts = defaultdict(int)\n",
      "for doc in alldicts:\n",
      "    for word,count in doc.items():\n",
      "        totcounts[word] += count\n",
      "sorted_words = sorted(totcounts.iteritems(),key=operator.itemgetter(1),reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute vocabulary and inverse vocabulary (string-to-index)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = {i:j for i,j in zip([x[0] for x in sorted_words[Dmin:Dmin+D]],range(D))}\n",
      "ivocab = {j:i for i,j in vocab.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build sparse matrix for data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_dok = dok_matrix((N,D))\n",
      "for i,doc in enumerate(alldicts):\n",
      "    for word,count in doc.items():\n",
      "        if word in vocab:\n",
      "            x_dok[i,vocab[word]] = count\n",
      "x = x_dok.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utility functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalizeRows(x):\n",
      "    row_sums = x.sum(axis=1)\n",
      "    return x / row_sums[:,np.newaxis]\n",
      "\n",
      "# can probably improve this\n",
      "def logNormalizeRows(x):\n",
      "    out = np.zeros(x.shape)\n",
      "    for i,xi in enumerate(x): #rows\n",
      "        out[i,:] = xi - logsumexp(xi)\n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Expectation maximization steps\n",
      "\n",
      "$\\phi \\propto q(y)' \\vec{x} + \\alpha$\n",
      "\n",
      "$\\theta \\propto \\sum q(y)$\n",
      "\n",
      "$q(y) \\propto \\exp \\left( (\\log \\phi)' \\vec{x} + \\log\\theta \\right)$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mStep(x,qy,alpha=alpha):\n",
      "    theta = np.log(qy.sum(axis=0)) #just add 'em up\n",
      "    phi = np.log(normalizeRows(x.T.dot(qy).T+alpha))\n",
      "    return phi,theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def eStep(x,phi,theta):\n",
      "    qy = np.exp(logNormalizeRows(x.dot(phi.T) + theta))\n",
      "    return qy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run it\n",
      "\n",
      "Initialize with random $q(y)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#initialize\n",
      "qy = np.random.rand(N,K)\n",
      "phi,theta = mStep(x,qy,alpha = .1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#show status\n",
      "plt.scatter(phi[0,:],phi[1,:])\n",
      "plt.xlabel('cluster 1')\n",
      "plt.ylabel('cluster 2')\n",
      "print qy.sum(axis=0)\n",
      "print [ivocab[i] for i in (phi[1,:] - phi[0,:]).argsort()[::-1][:10]]\n",
      "print [ivocab[i] for i in (phi[0,:] - phi[1,:]).argsort()[::-1][:10]]\n",
      "\n",
      "#do iteration\n",
      "qy = eStep(x,phi,theta)\n",
      "phi,theta = mStep(x,qy,alpha=.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 52.20950134  49.50083156]\n",
        "['roberts', 'hans', 'mercer', 'cooling', 'activity', 'bars', 'cuba', 'tim', 'staining', 'theresa']\n",
        "['hal', 'tom', 'shorts', 'downtown', 'andy', 'lord', 'eddie', 'phil', 'martin', 'barton']\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(20):\n",
      "    print i, qy.sum(axis=0), '-------------'\n",
      "    print [ivocab[i] for i in (phi[1,:] - phi[0,:]).argsort()[::-1][:10]]\n",
      "    print [ivocab[i] for i in (phi[0,:] - phi[1,:]).argsort()[::-1][:10]]\n",
      "    print ''\n",
      "    #do iteration\n",
      "    qy = eStep(x,phi,theta)\n",
      "    phi,theta = mStep(x,qy,alpha=.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 [ 68.21255989  42.78744011] -------------\n",
        "['staining', 'winston', 'cells', 'chlorine', 'drill', 'faculty', 'shayne', 'mercer', 'theresa', 'tim']\n",
        "['phil', 'wright', 'coating', 'foam', '**zg', 'miriam', 'andy', 'poet', 'forest', 'baseball']\n",
        "\n",
        "1 [ 70.99873493  40.00126507] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'tim']\n",
        "['phil', 'music', 'wright', 'coating', 'poet', 'foam', '**zg', 'andy', 'miriam', 'downtown']\n",
        "\n",
        "2 [ 71.0418819  39.9581181] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'coating', 'poet', 'foam', '**zg', 'andy', 'miriam', 'forest', 'downtown']\n",
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [ 71.00991969  39.99008031] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'miriam', 'andy', 'forest', 'downtown']\n",
        "\n",
        "4 [ 71.06673655  39.93326345] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'coating', 'poet', 'foam', '**zg', 'miriam', 'andy', 'downtown', 'forest']\n",
        "\n",
        "5 [ 71.99879879  39.00120121] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [ 71.99945961  39.00054039] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "7 [ 71.99944421  39.00055579] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "8 [ 71.99944335  39.00055665] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "9 [ 71.9994433  39.0005567]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "10 [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "11 [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "12 [ 71.9994433  39.0005567]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "13 [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "14 [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "16 [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "17 [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n",
        "19 [ 71.9994433  39.0005567] -------------\n",
        "['cells', 'staining', 'winston', 'chlorine', 'drill', 'shayne', 'tissue', 'mercer', 'theresa', 'cooling']\n",
        "['phil', 'music', 'poet', 'coating', 'foam', '**zg', 'andy', 'miriam', 'downtown', 'forest']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 116
    }
   ],
   "metadata": {}
  }
 ]
}